{
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "87393b85a9e84f5acf85b4897642fb1f420cfcaf148d9618810d560b47e94533"
   }
  },
  "lastEditStatus": {
   "notebookId": "b577zoocw522lcua6l7b",
   "authorId": "492945067848",
   "authorName": "SFACCHINE",
   "authorEmail": "saad.facchine@snowflake.com",
   "sessionId": "69b6df98-065d-407d-ac62-9f622a5e0344",
   "lastEditTime": 1742420681590
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "99fbd378-6297-420c-811e-d93eb47dca58",
   "metadata": {
    "name": "Use_Case_10",
    "collapsed": false
   },
   "source": "# Use Case 10\n\nDescription: Demonstrate how we could train and Deploy a model via notebook and/or script that leverages specified versions of python libraries.\n\n\n\n\n**This is an introduction to Snowpark for Snowflake.\nWe will use Snowpark to:**\n\n- Explore the data\n- Perform feature engineering\n- Train a model\n- Deploy the model in Snowflake\n\n\n**Why Snowpark?**\n\n- No copies or movement of data\n- Maintain governance\n- Leverage Snowflake scalable compute\n- ...and more!\n"
  },
  {
   "cell_type": "markdown",
   "id": "ce110000-1111-2222-3333-ffffff000000",
   "metadata": {
    "name": "add_packages"
   },
   "source": [
    "# Before you begin\n",
    "\n",
    "You will be running this as a Snowflake Notebook, so you will need to add the following packages in the Packages drop-down menu in the upper right corner of the UI:\n",
    "- matplotlib \n",
    "- plotly\n",
    "- snowflake-ml-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede8aebb-1df7-4bb7-8e96-52cdde843707",
   "metadata": {
    "name": "SNOWPARK_101"
   },
   "source": [
    "--------------\n",
    "# ❄️ SNOWPARK 101 ❄️\n",
    "--------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b340af82-63a2-4f5d-b8a4-8eafa6d8a26f",
   "metadata": {
    "name": "cell4"
   },
   "source": [
    "\n",
    "Let's get to know Snowpark. We will see that Snowpark makes it easy for Python users to leverage the Snowflake platform. Bringing these users into the Snowflake platform will foster collaboration and  streamline architecture across all users and teams."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a3011770-8623-4349-95e0-65b5051f53eb",
   "metadata": {
    "name": "cell5"
   },
   "source": [
    "## Import Packages\n",
    "Just like the Python packages we are importing, we will import the Snowpark modules that we need.<br>\n",
    "**Value**: Snowflake modules provide efficient ways to work with data and functions in Snowflake. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd87baf3-83da-4698-889f-b7e760acea51",
   "metadata": {
    "language": "python",
    "name": "imports"
   },
   "outputs": [],
   "source": "# Import Python packages\nimport pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\nimport matplotlib.pyplot as plt\nimport json\nimport sys\nimport cachetools\nfrom datetime import timedelta\nimport math\n\n# Import Snowflake modules\nfrom snowflake.snowpark import Session\nimport snowflake.snowpark.functions as F\nimport snowflake.snowpark.types as T\nfrom snowflake.snowpark import Window\nfrom snowflake.ml.modeling.impute import SimpleImputer\nfrom snowflake.ml.modeling.preprocessing import OneHotEncoder\nfrom snowflake.ml.modeling.pipeline import Pipeline\nfrom snowflake.ml.modeling.linear_model import LinearRegression\nfrom snowflake.ml.registry import Registry\nfrom snowflake.ml.model import type_hints\nfrom snowflake.ml.modeling.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error\n\n# For display purposes, suppress FutureWarnings and UserWarnings from printing below the cells\nimport warnings\nfor warning_category in [FutureWarning, UserWarning]:\n    warnings.filterwarnings('ignore', category=warning_category)"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "906bfc65-cf90-498c-8128-170355082727",
   "metadata": {
    "name": "cell7"
   },
   "source": [
    "## Connect to Snowflake\n",
    "Our Snowflake role, **tasty_bytes_ds_role**, can access the data in the **analytics** schema of the **frostbyte_tasty_bytes_dev** database. We will use the **tasty_dsci_wh** warehouse that has been created as a dedicated compute for data science workloads.\n",
    "\n",
    "**Value:** Secure and governed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "addad16c-ac0c-4951-8555-d8659737cd65",
   "metadata": {
    "language": "python",
    "name": "create_session"
   },
   "outputs": [],
   "source": [
    "# Get active session \n",
    "from snowflake.snowpark.context import get_active_session\n",
    "session = get_active_session()\n",
    "\n",
    "# Add a query tag to the session. This helps with debugging and performance monitoring.\n",
    "session.query_tag = {\"origin\":\"sf_sit\", \"name\":\"snowpark_101_ds\", \"version\":{\"major\":2, \"minor\":0},\"attributes\":{\"is_quickstart\":1, \"source\":\"notebook\"}}\n",
    "\n",
    "# Print the current role, warehouse, and database/schema\n",
    "print(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3189407-20ae-4fea-8b75-5e7c398f672c",
   "metadata": {
    "name": "cell10"
   },
   "source": [
    "The context of the connection created above will have the role, warehouse, database, and schema that were specified when creating this Snowflake Notebook. \n",
    "\n",
    "If we want to change any aspects of that context, we can use Session methods as seen in the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5070218b-7591-4609-b90b-40a4a8a777f4",
   "metadata": {
    "language": "python",
    "name": "change_context"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: \"TASTY_BYTES_DS_ROLE\" | WH: \"TASTY_DSCI_WH\" | DB.SCHEMA: \"FROSTBYTE_TASTY_BYTES_DEV\".\"ANALYTICS\"\n"
     ]
    }
   ],
   "source": "# Set session context\nsession.use_role('tasty_bytes_ds_role') \nsession.use_database('frostbyte_tasty_bytes_dev')\nsession.use_schema(\"analytics\")\nsession.use_warehouse(\"tasty_dsci_wh\")\n\nprint(f\"role: {session.get_current_role()} | WH: {session.get_current_warehouse()} | DB.SCHEMA: {session.get_fully_qualified_current_schema()}\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2c91df24-38b1-4584-8422-c0a2a5e386da",
   "metadata": {
    "name": "cell12"
   },
   "source": [
    "## Snowpark DataFrame\n",
    "Let's create a Snowpark DataFrame containing our shift sales data from the **shift_sales_v** view in our Snowflake account using the Snowpark session.table function. A DataFrame is a data structure that contains rows and columns, similar to a SQL table.<br>\n",
    "**Value:** Familiar representation of data for Python users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85142cce-9541-4808-979f-37bca023c74c",
   "metadata": {
    "language": "python",
    "name": "initial_df"
   },
   "outputs": [],
   "source": "snowpark_df = session.table(\"frostbyte_tasty_bytes_dev.analytics.shift_sales_v\")"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "416ba257-fe30-4023-b48d-f1732808c2a0",
   "metadata": {
    "name": "cell14"
   },
   "source": [
    "## Preview the Data\n",
    "With our Snowpark DataFrame defined, let’s use the .show() function to take a look at the first 10 rows. <br>\n",
    "**Value:** Instant access to data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "885fc013-037b-4b37-beb4-7f7fc869c309",
   "metadata": {
    "language": "python",
    "name": "show_df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"LOCATION_ID\"  |\"CITY\"         |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"CITY_POPULATION\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"LOCATION_NAME\"               |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2574           |New York City  |2023-05-02  |NULL           |AM       |5        |2              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-05  |NULL           |AM       |5        |5              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-01  |NULL           |AM       |5        |1              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-07  |NULL           |AM       |5        |0              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-04  |NULL           |AM       |5        |4              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-03  |NULL           |AM       |5        |3              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|2574           |New York City  |2023-05-06  |NULL           |AM       |5        |6              |8804190            |40.731148   |-74.003081   |Faicco's Italian Specialties  |\n",
      "|6461           |Cairo          |2023-05-02  |NULL           |PM       |5        |2              |1010166            |30.041063   |31.207879    |World Gym                     |\n",
      "|6461           |Cairo          |2023-05-05  |NULL           |PM       |5        |5              |1010166            |30.041063   |31.207879    |World Gym                     |\n",
      "|6461           |Cairo          |2023-05-01  |NULL           |PM       |5        |1              |1010166            |30.041063   |31.207879    |World Gym                     |\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snowpark_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae86989e-b455-48c8-81d2-122ec6300c30",
   "metadata": {
    "name": "Use_Case_11",
    "collapsed": false
   },
   "source": "\n# Use Case 11\n--------------\n# ❄️ FEATURE ENGINEERING ❄️ \n--------------"
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6070f262-a805-4833-819d-f5e702cfa227",
   "metadata": {
    "name": "cell40"
   },
   "source": [
    "We will create a feature engineering pipeline by chaining transformations to prepare our data for model training.<br>\n",
    "**Value:** The Snowpark syntax makes pipelines easy to implement and understand. The syntax also allows for easy migration of Spark pipelines to Snowflake.<br>\n",
    "**All transformations for feature engineering in this notebook will be executed on Snowflake compute.**\n",
    "\n",
    "Notice what we haven't had to do? No tuning, maintenance, or operational overhead. We just need a role, warehouse, and access to the data.<br>\n",
    "**Value**: Near-zero maintenance. Focus on the work that brings value."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d30d9c54-114e-4cbe-8cc1-e107c8cb1838",
   "metadata": {
    "name": "cell41"
   },
   "source": [
    "## Create a Rolling Average Feature\n",
    "We will use a Snowflake window function to get a **rolling shift average by location** over time. Window functions allow us to aggregate on a \"moving\" group of rows.\n",
    "\n",
    "**Step 1. Create a Window**\n",
    "\n",
    "Our window will partition the data by location and shift. It will order rows by date. It will include all rows prior to the current date of the observation it is aggregating for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a3b2eec0-a63f-464e-9e2f-ca928c9ab0b3",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "create_window"
   },
   "outputs": [],
   "source": [
    "window_by_location_all_days = (\n",
    "    Window.partition_by(\"location_id\", \"shift\")\n",
    "    .order_by(\"date\")\n",
    "    .rows_between(Window.UNBOUNDED_PRECEDING, Window.CURRENT_ROW - 1)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5b9f50a1-0830-4c1c-ada8-ddc88dc4d366",
   "metadata": {
    "name": "cell43"
   },
   "source": [
    "**Step 2. Aggregate across the Window**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "123d4094-5cac-4c5b-bf2c-ac358d3b9ad7",
   "metadata": {
    "collapsed": false,
    "language": "python",
    "name": "aggregate_across_window"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"LOCATION_ID\"  |\"CITY\"   |\"DATE\"      |\"SHIFT_SALES\"  |\"SHIFT\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"CITY_POPULATION\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"LOCATION_NAME\"        |\"AVG_LOCATION_SHIFT_SALES\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|1303           |Toronto  |2015-04-06  |298.91         |PM       |4        |1              |2794356            |43.629078   |-79.581193   |Golf Canada SCOREGolf  |NULL                        |\n",
      "|1303           |Toronto  |2015-05-30  |550.72         |PM       |5        |6              |2794356            |43.629078   |-79.581193   |Golf Canada SCOREGolf  |298.91                      |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "snowpark_df = snowpark_df.with_column(\n",
    "    \"avg_location_shift_sales\", \n",
    "    F.avg(\"shift_sales\").over(window_by_location_all_days)\n",
    ")\n",
    "\n",
    "snowpark_df.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05991ec7-af38-4c2c-b8b6-888925738be9",
   "metadata": {
    "name": "MODEL_TRAINING"
   },
   "source": [
    "--------------\n",
    "# ❄️ MODEL TRAINING ❄️ \n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2dedd8-f9f4-40c5-9a28-3cf090c7a0cf",
   "metadata": {
    "name": "cell46"
   },
   "source": [
    "## Filter to Historical Data\n",
    "Our model will provide the predicted sales at each location for the upcoming shift (\"AM\" or \"PM\").\n",
    "\n",
    "Our data includes placeholders for future data with missing **shift_sales**. The **future data** represents the next 7 days of shifts for all locations. The **historical data** has shift sales for all locations where a food truck parked during a shift. We will only use historical data when training our model and will filter out future data (i.e. the dates where the **shift_sales** column is missing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7fb9cc3c-1991-4a70-b64b-676d3bb4b8f2",
   "metadata": {
    "language": "python",
    "name": "create_historical_df"
   },
   "outputs": [],
   "source": [
    "historical_snowpark_df = snowpark_df.filter(F.col(\"shift_sales\").is_not_null())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67985e55-bdbb-480e-b6de-c1a55f5464ba",
   "metadata": {
    "name": "cell48"
   },
   "source": [
    "## Split data into training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5083697-3917-40d6-b91a-d7860941030c",
   "metadata": {
    "name": "cell49"
   },
   "source": [
    "We will initially train our model on all historical data except the most recent year, which we will reserve as a test dataset. The test dataset will be used to evaluate how the model might perform on future data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "30c9d4aa-7502-4e1b-a359-3b3d6712a618",
   "metadata": {
    "language": "python",
    "name": "train_test_split"
   },
   "outputs": [],
   "source": [
    "# Obtain the min and max dates present in the historical DataFrame\n",
    "date_range = historical_snowpark_df.agg(F.min(\"DATE\").alias(\"MIN_DATE\"), F.max(\"DATE\").alias(\"MAX_DATE\")).collect()[0]\n",
    "min_date = date_range['MIN_DATE']\n",
    "max_date = date_range['MAX_DATE']\n",
    "\n",
    "# Obtain the start date for the test set (365 days before the max date)\n",
    "test_start_date = max_date - timedelta(days=365)\n",
    "\n",
    "# Create train and test sets (dropping the date column) \n",
    "train_snowpark_df = historical_snowpark_df.filter(F.col(\"DATE\") < test_start_date).drop(\"DATE\")\n",
    "test_snowpark_df = historical_snowpark_df.filter(F.col(\"DATE\") >= test_start_date).drop(\"DATE\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01878f82-6df0-4303-8986-ce86c7f6d3f8",
   "metadata": {
    "name": "cell51"
   },
   "source": [
    "## Build a Snowpark ML Pipeline\n",
    "\n",
    "**Snowpark ML** is an integrated set of capabilities for end-to-end machine learning in a single platform on top of your governed data.\n",
    "\n",
    "In the next cell we will use a Pipeline to build our model. A Pipeline is a tool that sequentially applies a list of transforms and a final estimator, streamlining the process of preprocessing and modeling by combining these steps into a single object. \n",
    "\n",
    "This Pipeline first performs **Preprocessing** steps using Snowpark ML pre-processors (SimpleImputer and OneHotEncoder). These steps will replace any missing values in the **avg_location_shift_sales** column with 0. It will also convert the categorical column **SHIFT** into a numeric column with a value of 1 for \"AM\" shifts and 0 for \"PM\" shifts. \n",
    "\n",
    "Next the Pipeline trains a Snowpark ML **LinearRegression** model using the newly-transformed features and any other features that already existed in the dataset. Features listed in the **passthrough_cols** argument will not be used to train the model. \n",
    "\n",
    "Linear regression finds a line that best fits the data points used in training. We then use that line as an estimation of where output values will be for future scenarios. Here, training will use historical shift sales and features in our data to predict future shift sales of locations where our food trucks can park."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "64940490-1e1d-415f-9f85-b0d0b4d1b4c7",
   "metadata": {
    "language": "python",
    "name": "create_model_pipeline"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been fit to training dataset.\n"
     ]
    }
   ],
   "source": [
    "# Build the pipeline\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\n",
    "            \"SimpleImputer (numeric)\",\n",
    "            SimpleImputer(\n",
    "                input_cols=[\"AVG_LOCATION_SHIFT_SALES\"],\n",
    "                output_cols=[\"AVG_LOCATION_SHIFT_SALES\"],\n",
    "                strategy=\"constant\",\n",
    "                fill_value=0\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"OneHotEncoder\",\n",
    "            OneHotEncoder(\n",
    "                drop=[\"PM\"],\n",
    "                handle_unknown=\"ignore\",\n",
    "                input_cols=[\"SHIFT\"],\n",
    "                output_cols=[\"SHIFT\"],\n",
    "            ),\n",
    "        ),\n",
    "        (\n",
    "            \"LinearRegression\",\n",
    "            LinearRegression(\n",
    "                label_cols=[\"SHIFT_SALES\"],\n",
    "                passthrough_cols=['LOCATION_ID', 'CITY', 'DATE', 'SHIFT', 'LOCATION_NAME'],\n",
    "                output_cols=[\"FORECASTED_SHIFT_SALES\"],\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "# We will remove the first column which contains nulls from the rolling average\n",
    "pipeline.fit(train_snowpark_df.limit(train_snowpark_df.count() - 1, 1))\n",
    "\n",
    "print(\"Model has been fit to training dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca40a5f9-fc14-4220-b10b-6fce438c7df8",
   "metadata": {
    "name": "cell53"
   },
   "source": [
    "## Examine Feature Contributions\n",
    "The model object from our fitted Pipeline contains model coefficient estimates, which can be interepreted as feature contributions. Here we temporarily convert the LinearRegression model (the last step of the fitted Pipeline) to an **scikit-learn** verison of the model. This will allow us to access the **coef_** attribute of the model object.\n",
    "\n",
    "We can see which features have the largest impact on shift sale predictions. The prediction provided by the linear regression model is a summation of the feature values multiplied by their respective coefficients (plus an additional bias term). We can see that there is a large negative coefficient on the SHIFT_AM feature, which suggests that people are hungrier in the afternoon! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "243afc28-ced7-442b-b4da-077eb82b897a",
   "metadata": {
    "language": "python",
    "name": "inspect_coefficients"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Weight</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SHIFT_AM</td>\n",
       "      <td>-18.769728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MONTH</td>\n",
       "      <td>-6.526725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DAY_OF_WEEK</td>\n",
       "      <td>2.733950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AVG_LOCATION_SHIFT_SALES</td>\n",
       "      <td>0.980475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LATITUDE</td>\n",
       "      <td>-0.321237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LONGITUDE</td>\n",
       "      <td>0.106510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CITY_POPULATION</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    Feature     Weight\n",
       "0                  SHIFT_AM -18.769728\n",
       "2                     MONTH  -6.526725\n",
       "3               DAY_OF_WEEK   2.733950\n",
       "1  AVG_LOCATION_SHIFT_SALES   0.980475\n",
       "5                  LATITUDE  -0.321237\n",
       "6                 LONGITUDE   0.106510\n",
       "4           CITY_POPULATION   0.000008"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Can use to_sklearn() method to convert the Snowflake model to a scikit-learn model\n",
    "model_step = pipeline.steps[-1][1].to_sklearn()\n",
    "\n",
    "# Create a dataframe of features and their coefficients (weights)\n",
    "feature_weights = pd.DataFrame({\"Feature\": model_step.feature_names_in_, \"Weight\": model_step.coef_})\n",
    "\n",
    "feature_weights.sort_values(\"Weight\", key=abs, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9aec9c-8e96-44bd-8517-a26bda3fe895",
   "metadata": {
    "name": "cell55"
   },
   "source": [
    "## Evaluate model performance\n",
    "To get an idea of how well our model will generalize to future unseen datasets, we will obtain predictions on the test dataset (which was not seen by the model during training) and compare those predictions to the actual values. The performance metrics we calculate can be used when comparing two different models to determine which one will potentially perform better in production. \n",
    "\n",
    "First we will predict on the test set. Then we will use Snowpark ML to calculate a handful of standard metrics used to evaluate regression models. For example, **mean absolute error (MAE)** lets us know how far on average our predictions are from the actual values. \n",
    "\n",
    "The metrics obtained here will be registered to the Model Registry when we deploy the model later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ce80b442-881d-4707-9d13-c092626f5920",
   "metadata": {
    "language": "python",
    "name": "evaluate_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------\n",
      "|\"LOCATION_ID\"  |\"LOCATION_NAME\"          |\"SHIFT\"  |\"SHIFT_SALES\"  |\"FORECASTED_SHIFT_SALES\"  |\n",
      "------------------------------------------------------------------------------------------------\n",
      "|7906           |Four Points By Sheraton  |PM       |1045.13        |1056.7878763781996        |\n",
      "|7906           |Four Points By Sheraton  |PM       |760.43         |1068.2330499354484        |\n",
      "|7906           |Four Points By Sheraton  |PM       |1154.48        |1049.9126419768172        |\n",
      "------------------------------------------------------------------------------------------------\n",
      "\n",
      "{'rmse': 375.8557, 'mae': 261.9697, 'mape': 0.3354}\n"
     ]
    }
   ],
   "source": [
    "# Obtain predictions on the test set\n",
    "preds_test = pipeline.predict(test_snowpark_df)\n",
    "\n",
    "# Show the first few rows of predictions\n",
    "preds_test.select(\"LOCATION_ID\", \"LOCATION_NAME\", \"SHIFT\", \"SHIFT_SALES\", \"FORECASTED_SHIFT_SALES\").show(3)\n",
    "\n",
    "# Calculate Root Mean Square Error (RMSE)\n",
    "mse = mean_squared_error(df=preds_test, y_true_col_names=\"SHIFT_SALES\", y_pred_col_names=\"FORECASTED_SHIFT_SALES\")\n",
    "rmse = round(math.sqrt(mse), 4)\n",
    "\n",
    "# Calculate Mean Absolute Error (MAE)\n",
    "mae = mean_absolute_error(df=preds_test, y_true_col_names=\"SHIFT_SALES\", y_pred_col_names=\"FORECASTED_SHIFT_SALES\")\n",
    "mae = round(mae, 4)\n",
    "\n",
    "# Calculate Mean Absolute Percentage Error (MAPE)\n",
    "mape = mean_absolute_percentage_error(df=preds_test, y_true_col_names=\"SHIFT_SALES\", y_pred_col_names=\"FORECASTED_SHIFT_SALES\")\n",
    "mape = round(mape, 4)\n",
    "\n",
    "# Create a dictionary containing all the metrics, which will be registered along with the model\n",
    "test_metrics = {\"rmse\": rmse, \"mae\": mae, \"mape\": mape}\n",
    "print(test_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d046ceb-3783-46a7-91fa-380b34969963",
   "metadata": {
    "name": "cell57"
   },
   "source": [
    "## Fit the pipeline to the entire historical set\n",
    "\n",
    "Now that we have developed our model and obtained performance metrics on our test set, we can fit the model on the entire historical dataset to ensure that the deployed model will have been trained on the most recent data. We will still use the Pipeline we built above, but we will fit it on a different dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dad92241-0339-49b1-8a54-10e7a9c188ff",
   "metadata": {
    "language": "python",
    "name": "fit_final_model"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has been trained on all historical data\n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(historical_snowpark_df.limit(historical_snowpark_df.count() - 1, 1).drop(\"DATE\"))\n",
    "\n",
    "print(\"Model has been trained on all historical data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70da5804-3b23-4c2a-91a6-eade32663c59",
   "metadata": {
    "name": "Use_Case_12",
    "collapsed": false
   },
   "source": "# Use Case 12\n--------------\n# ❄️ MODEL DEPLOYMENT ❄️ \n--------------"
  },
  {
   "cell_type": "markdown",
   "id": "e0635cbd-c77e-4025-b623-2fab9e814591",
   "metadata": {
    "name": "cell60"
   },
   "source": [
    "## Log the model in the Model Registry\n",
    "In ML, a model registry is a centralized repository or model store, similar to a library, that lets you effectively manage and organize machine learning models. It is where models are stored, tracked, versioned, and made accessible to anyone at the company involved in deploying and using models in production.\n",
    "\n",
    "Snowflake provides Model Registry capabilities that will allow you to store your models and model versions in a common location along with metadata related to each model. This greatly streamlines model deployment. Through the Model Registry, Snowflake handles storing the model object and making the model available for inference. \n",
    "\n",
    "Model Registries in Snowflake are schema-level objects, so an account can have a single model registry or several model registries for different purposes/business units, depending on the needs of the organization. If a Registry does not already exist in a schema, then one will be created the first time you log a model. \n",
    "\n",
    "Here we establish a Registry object, and then use the **log_model** method to place our fitted Pipeline into the Registry along with the evaluation metrics we obtained above. We name the model \"**shift_sales_forecaster**\", and any time we re-train and log a model with that name, the Registry will store it as a new version. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "da1bcb8e-be88-415a-93a3-82657336af5e",
   "metadata": {
    "language": "python",
    "name": "register_model"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>comment</th>\n",
       "      <th>owner</th>\n",
       "      <th>default_version_name</th>\n",
       "      <th>versions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-28 12:10:21.893000-07:00</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>None</td>\n",
       "      <td>TASTY_BYTES_DS_ROLE</td>\n",
       "      <td>STUPID_MOOSE_4</td>\n",
       "      <td>[\"BITTER_BAT_4\",\"GENTLE_BOBCAT_2\",\"GIANT_LEECH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on                    name  \\\n",
       "0 2024-06-28 12:10:21.893000-07:00  SHIFT_SALES_FORECASTER   \n",
       "\n",
       "               database_name schema_name comment                owner  \\\n",
       "0  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS    None  TASTY_BYTES_DS_ROLE   \n",
       "\n",
       "  default_version_name                                           versions  \n",
       "0       STUPID_MOOSE_4  [\"BITTER_BAT_4\",\"GENTLE_BOBCAT_2\",\"GIANT_LEECH...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": "reg = Registry(session)\nreg.log_model(model=pipeline, model_name=\"shift_sales_forecaster\", metrics=test_metrics, task = type_hints.Task.TABULAR_REGRESSION)\n\n# Show all models in the registry\nreg.show_models()"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8728c130-ff7c-44c4-9be5-10ece7c4537c",
   "metadata": {
    "language": "python",
    "name": "show_model_versions"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_on</th>\n",
       "      <th>name</th>\n",
       "      <th>aliases</th>\n",
       "      <th>comment</th>\n",
       "      <th>database_name</th>\n",
       "      <th>schema_name</th>\n",
       "      <th>module_name</th>\n",
       "      <th>is_default_version</th>\n",
       "      <th>functions</th>\n",
       "      <th>metadata</th>\n",
       "      <th>user_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-06-28 12:10:21.943000-07:00</td>\n",
       "      <td>STUPID_MOOSE_4</td>\n",
       "      <td>[\"DEFAULT\",\"FIRST\"]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>true</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-06-28 12:12:41.551000-07:00</td>\n",
       "      <td>UGLY_ROBIN_3</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-07-02 05:55:37.542000-07:00</td>\n",
       "      <td>BITTER_BAT_4</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-07-02 06:29:20.995000-07:00</td>\n",
       "      <td>GENTLE_BOBCAT_2</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-02 06:45:31.906000-07:00</td>\n",
       "      <td>MODERN_FIREFOX_3</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-07-02 07:55:01.386000-07:00</td>\n",
       "      <td>LOUD_FALCON_4</td>\n",
       "      <td>[\"LAST\"]</td>\n",
       "      <td>None</td>\n",
       "      <td>FROSTBYTE_TASTY_BYTES_DEV</td>\n",
       "      <td>ANALYTICS</td>\n",
       "      <td>SHIFT_SALES_FORECASTER</td>\n",
       "      <td>false</td>\n",
       "      <td>[\"PREDICT\"]</td>\n",
       "      <td>{\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...</td>\n",
       "      <td>{}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        created_on              name              aliases  \\\n",
       "0 2024-06-28 12:10:21.943000-07:00    STUPID_MOOSE_4  [\"DEFAULT\",\"FIRST\"]   \n",
       "1 2024-06-28 12:12:41.551000-07:00      UGLY_ROBIN_3                   []   \n",
       "2 2024-07-02 05:55:37.542000-07:00      BITTER_BAT_4                   []   \n",
       "3 2024-07-02 06:29:20.995000-07:00   GENTLE_BOBCAT_2                   []   \n",
       "4 2024-07-02 06:45:31.906000-07:00  MODERN_FIREFOX_3                   []   \n",
       "5 2024-07-02 07:55:01.386000-07:00     LOUD_FALCON_4             [\"LAST\"]   \n",
       "\n",
       "  comment              database_name schema_name             module_name  \\\n",
       "0    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "1    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "2    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "3    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "4    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "5    None  FROSTBYTE_TASTY_BYTES_DEV   ANALYTICS  SHIFT_SALES_FORECASTER   \n",
       "\n",
       "  is_default_version    functions  \\\n",
       "0               true  [\"PREDICT\"]   \n",
       "1              false  [\"PREDICT\"]   \n",
       "2              false  [\"PREDICT\"]   \n",
       "3              false  [\"PREDICT\"]   \n",
       "4              false  [\"PREDICT\"]   \n",
       "5              false  [\"PREDICT\"]   \n",
       "\n",
       "                                            metadata user_data  \n",
       "0  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  \n",
       "1  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  \n",
       "2  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  \n",
       "3  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  \n",
       "4  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  \n",
       "5  {\"metrics\": {\"rmse\": 375.8557, \"mae\": 261.9697...        {}  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Show all versions of a specific model (\"SHIFT_SALES_FORECASTER\" in this case)\n",
    "reg.get_model(\"SHIFT_SALES_FORECASTER\").show_versions()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112312a5-90d5-415e-a324-4cbc89dfb194",
   "metadata": {
    "name": "cell2",
    "collapsed": false
   },
   "source": "# Enable Monitoring\nCreate a model monitor using the CREATE MODEL MONITOR command. The monitor object automatically refreshes the monitor logs by querying source data and updates the monitoring reports based on the logs."
  },
  {
   "cell_type": "code",
   "id": "40a8c013-7ff1-47bb-881a-efe0f5237b1a",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddfe939a-1623-4828-aeff-06479354a232",
   "metadata": {
    "language": "python",
    "name": "cell1"
   },
   "outputs": [],
   "source": "query = f\"\"\"\nCREATE OR REPLACE MODEL MONITOR SALES_MODEL_MONITOR\nWITH\n    MODEL=SHIFT_SALES_FORECASTER\n    VERSION=TOUGH_EEL_4\n    FUNCTION=predict\n    SOURCE=shift_sales_groundtruth\n    BASELINE=shift_sales_baseline\n    TIMESTAMP_COLUMN=created_at\n    PREDICTION_SCORE_COLUMNS=(FORECASTED_SHIFT_SALES)  \n    ACTUAL_SCORE_COLUMNS=(SHIFT_SALES)\n    ID_COLUMNS=(LOCATION_ID)\n    WAREHOUSE=TASTY_DSCI_WH\n    REFRESH_INTERVAL='10 min'\n    AGGREGATION_WINDOW='1 day';\n\"\"\"\nsession.sql(query).collect()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7b3b72fb-695a-4449-a40a-349e6b2a5f78",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Automated retraining\n\nBelow is a code snippet and example of creating a DAG on Snowflake that does a daily retraining job."
  },
  {
   "cell_type": "code",
   "id": "23689288-8ca4-4511-9d83-aa45b45e5fcb",
   "metadata": {
    "language": "python",
    "name": "cell9"
   },
   "outputs": [],
   "source": "api_root = Root(session)\nschema = api_root.databases[DB_NAME].schemas[SCHEMA_NAME]\nSTAGE_LOCATION=f\"@{DB_NAME}.{SCHEMA_NAME}.SPROC_STAGE\"\n\ndag = DAG(\"daily_training\",\n          schedule=timedelta(days=1),\n          use_func_return_value=True,\n          warehouse=WAREHOUSE,\n          stage_location=f\"{STAGE_LOCATION}\",\n          packages=[\"snowflake-snowpark-python\", \"snowflake-ml-python\", \"xgboost\"],\n          config={'DB_NAME': DB_NAME, 'training_data_table_name': 'training_data', 'test_data_table_name': 'training_data'},\n          )\nwith dag:\n    setup_task = DAGTask(\"setup\", definition=setup)\n    condition = DAGTaskBranch(\"condition\", definition=should_push_to_prod)\n    mark_model_golden_task = DAGTask(\"mark_model_golden\", definition=mark_model_golden)\n    cleanup_task = DAGTask(\"cleanup_task\", definition=cleanup)\n\ndag_op = DAGOperation(schema)\ndag_op.deploy(dag, mode=\"orReplace\")\ndag_op.run(dag)",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "2b89a3cc-c88f-4843-9eb0-443f276fcca5",
   "metadata": {
    "name": "INFERENCE"
   },
   "source": [
    "--------------\n",
    "# ❄️ INFERENCE ❄️ \n",
    "--------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c46ff3-4ba4-47ee-a829-07e29f783f27",
   "metadata": {
    "name": "cell64"
   },
   "source": [
    "## Predict for tomorrow morning's shift in Vancouver\n",
    "\n",
    "Now that our model is deployed, we can use it in Snowflake on our data to get shift sales predictions. \n",
    "\n",
    "First we will filter to the morning shift of the first future date in our data and Vancouver locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "85d8535e-8470-46a1-9f69-7ca58cc71167",
   "metadata": {
    "language": "python",
    "name": "get_inference_df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"LOCATION_ID\"  |\"CITY\"     |\"SHIFT_SALES\"  |\"SHIFT\"  |\"LOCATION_NAME\"         |\"SHIFT_AM\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"CITY_POPULATION\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"FORECASTED_SHIFT_SALES\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2424           |Vancouver  |NULL           |AM       |Flowers by Michael      |1           |164.18241758241757          |5        |1              |662248             |49.288758   |-123.128241  |217.78042678838736        |\n",
      "|5192           |Vancouver  |NULL           |AM       |Dreamgroup Productions  |1           |167.91547619047617          |5        |1              |662248             |49.265897   |-123.134523  |221.88828776020088        |\n",
      "|3016           |Vancouver  |NULL           |AM       |lululemon athletica     |1           |192.6601818181818           |5        |1              |662248             |49.284272   |-123.124617  |249.08059677150058        |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the date to predict\n",
    "date_tomorrow = snowpark_df.filter(F.col(\"shift_sales\").is_null()).select(F.min(\"date\")).collect()[0][0]\n",
    "# Filter to tomorrow's date and the morning shift in Vancouver\n",
    "location_predictions_df = snowpark_df.filter((F.col(\"date\") == date_tomorrow) \n",
    "                                             & (F.col(\"shift\") == \"AM\") \n",
    "                                             & (F.col(\"city\")==\"Vancouver\")).drop(\"DATE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214c0b3-3a29-4ba1-8645-73c75bda67b2",
   "metadata": {
    "name": "cell66"
   },
   "source": [
    "Next we will access the model in the Model Registry. We use the **.default** attribute to specify that we would like to use the \"default\" version of the model. You can set any version to be the \"default\" version. Often the \"default\" version is chosen to be the most recent version of the model or the one with the best performance metric.  \n",
    "\n",
    "To obtain our predictions we simply set function_name=\"predict\" in the **run** method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "28078721-8f45-44b1-8aa1-ffd9a2d82ba2",
   "metadata": {
    "language": "python",
    "name": "make_predictions"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|\"LOCATION_ID\"  |\"CITY\"     |\"SHIFT_SALES\"  |\"SHIFT\"  |\"LOCATION_NAME\"         |\"SHIFT_AM\"  |\"AVG_LOCATION_SHIFT_SALES\"  |\"MONTH\"  |\"DAY_OF_WEEK\"  |\"CITY_POPULATION\"  |\"LATITUDE\"  |\"LONGITUDE\"  |\"FORECASTED_SHIFT_SALES\"  |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "|2636           |Vancouver  |NULL           |AM       |Central Parking System  |1           |173.6669911504424           |5        |1              |662248             |49.289678   |-123.128364  |228.20432191891774        |\n",
      "|2127           |Vancouver  |NULL           |AM       |Formally Kirkpatrick    |1           |131.90425000000002          |5        |1              |662248             |49.24916    |-123.146497  |182.31282606296742        |\n",
      "|4149           |Vancouver  |NULL           |AM       |Canada Place            |1           |201.51303921568623          |5        |1              |662248             |49.2887     |-123.11117   |258.8104283980143         |\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": "# Obtain predictions\ninference_df = reg.get_model(\"SHIFT_SALES_FORECASTER\").default.run(X=location_predictions_df, function_name=\"predict\")\ninference_df.show(3)\n\n"
  },
  {
   "cell_type": "code",
   "id": "96f4c534-e89b-45f3-a8c3-e696c73f20fb",
   "metadata": {
    "language": "sql",
    "name": "cell6"
   },
   "outputs": [],
   "source": "select * from shift_sales_groundtruth;",
   "execution_count": null
  }
 ]
}