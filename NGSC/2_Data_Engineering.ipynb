{
 "metadata": {
  "kernelspec": {
   "display_name": "Streamlit Notebook",
   "name": "streamlit"
  },
  "lastEditStatus": {
   "notebookId": "hj7meqqpth65qoqsxvv3",
   "authorId": "492945067848",
   "authorName": "SFACCHINE",
   "authorEmail": "saad.facchine@snowflake.com",
   "sessionId": "52a11ade-7291-4543-addc-9c26eb6e4458",
   "lastEditTime": 1742493250112
  }
 },
 "nbformat_minor": 5,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7a3f15a-5674-4fba-976d-a1b4f62c596f",
   "metadata": {
    "name": "cell11",
    "collapsed": false
   },
   "source": "### Use Case 2\nDescription: Demonstrate how to represent Data Dependencies in my pipelines in an intuitive and easy to understand way. We often have cases where multiple ETL jobs run in sequence across a set of tables, and tracking that lineage is difficult. If I modify something earlier in the chain, it can have knock on effects. Demonstrate how to run tests against a development branch that modifies and early object in a Data Dependency chain, and test that other inheriting jobs run successfully.\n\n\n\nDynamic Tables in Snowflake solve pipeline dependency challenges by:\n\n1. **Automatic Dependency Management**\n   - Self-maintaining tables that automatically update when source data changes\n   - Clearly defined lineage through SQL definition\n   - No need for complex scheduling or orchestration\n\n2. **Testing Changes Safely**\n   - Clone source tables to test modifications\n   - Dynamic tables automatically propagate changes through the chain\n   - Test entire pipeline without affecting production\n"
  },
  {
   "cell_type": "code",
   "id": "7fb343b0-1430-429b-a479-7acadde5ea0d",
   "metadata": {
    "language": "sql",
    "name": "cell1"
   },
   "outputs": [],
   "source": "-- Snowflake Data Engineering Demo SQL Worksheet\n\n-- =========================================\n-- 1. Setting Up the Environment\n-- =========================================\n-- Using the appropriate role and warehouse for this demo.\nUSE ROLE tasty_data_engineer;\nUSE WAREHOUSE tasty_de_wh;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fe435148-12e4-41e0-aebf-3e4ae09895c2",
   "metadata": {
    "name": "cell12",
    "collapsed": false
   },
   "source": "1. **Creation and Configuration**\n   - Dynamic tables are created using a CREATE DYNAMIC TABLE statement that specifies the target lag time (how often it should refresh), warehouse assignment (compute resources), and refresh mode (full or incremental)\n   - The table is defined by a SELECT query that pulls from one or more source tables, establishing the data transformation logic\n\n2. **Automated Maintenance**\n   - Once created, dynamic tables are completely self-maintaining, automatically detecting and processing changes from source tables without manual intervention or scheduling\n   - The system continuously monitors source data and ensures the dynamic table stays updated within the specified target lag time\n\n3. **Intelligent Processing**\n   - When running in incremental mode, only changed data is processed, making updates highly efficient\n   - The system tracks dependencies and automatically propagates changes through chains of dynamic tables\n   - Processing occurs on the assigned warehouse, optimizing resource usage\n\n4. **Simplified Pipeline Management**\n   - Eliminates need for complex scheduling and orchestration tools\n   - Provides clear visibility into data lineage through SQL definitions\n   - Reduces maintenance overhead and risk of pipeline failures\n   - Enables easy testing of changes through cloning and automatic propagation"
  },
  {
   "cell_type": "code",
   "id": "b7489191-c30f-4c92-a89a-674fe85304f6",
   "metadata": {
    "language": "sql",
    "name": "cell2"
   },
   "outputs": [],
   "source": "-- =========================================\n-- 2. Creating the Silver Layer with Dynamic Tables\n-- =========================================\n-- Dynamic Tables allow for continuous transformation without complex scheduling.\nCREATE OR REPLACE DYNAMIC TABLE FROSTBYTE_TASTY_BYTES.HARMONIZED.ORDERS_DT\nTARGET_LAG = 'DOWNSTREAM'\nWAREHOUSE = 'TASTY_DE_WH'\nREFRESH_MODE=INCREMENTAL\nAS \nSELECT \n    oh.order_id,\n    oh.truck_id,\n    oh.order_ts,\n    od.order_detail_id,\n    od.line_number,\n    od.menu_item_id,\n    od.quantity,\n    od.unit_price,\n    od.price,\n    oh.order_amount,\n    oh.order_tax_amount,\n    oh.order_discount_amount,\n    oh.order_total,\n    oh.location_id,\n    oh.customer_id\nFROM frostbyte_tasty_bytes.raw_pos.order_detail od\nJOIN frostbyte_tasty_bytes.raw_pos.order_header oh\n    ON od.order_id = oh.order_id;",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4688f288-666c-4ea6-862c-518adb8d6658",
   "metadata": {
    "name": "cell13",
    "collapsed": false
   },
   "source": "We can chain dynamic tables to create a medallion architecture within Snowflake. "
  },
  {
   "cell_type": "code",
   "id": "aabb2e97-0eff-4b27-9428-00134a5cb449",
   "metadata": {
    "language": "sql",
    "name": "cell3"
   },
   "outputs": [],
   "source": "-- =========================================\n-- 3. Enriching the Data â€“ Gold Layer\n-- =========================================\n-- Bringing in additional dimensional data without manual pipeline management.\nCREATE OR REPLACE DYNAMIC TABLE FROSTBYTE_TASTY_BYTES.HARMONIZED.ORDERS_ENRICHED_DT\nTARGET_LAG = '5 minutes'\nWAREHOUSE = 'TASTY_DE_WH'\nREFRESH_MODE=INCREMENTAL\nAS \nSELECT \n    s.*,\n    m.truck_brand_name,\n    m.menu_type,\n    m.menu_item_name,\n    t.primary_city,\n    t.region,\n    t.country,\n    t.franchise_flag,\n    t.franchise_id,\n    f.first_name AS franchisee_first_name,\n    f.last_name AS franchisee_last_name,\n    cl.first_name,\n    cl.last_name,\n    cl.e_mail,\n    cl.phone_number,\n    cl.children_count,\n    cl.gender,\n    cl.marital_status\nFROM FROSTBYTE_TASTY_BYTES.HARMONIZED.ORDERS_DT s\nJOIN frostbyte_tasty_bytes.raw_pos.truck t \n    ON s.truck_id = t.truck_id\nJOIN frostbyte_tasty_bytes.raw_pos.menu m \n    ON s.menu_item_id = m.menu_item_id\nJOIN frostbyte_tasty_bytes.raw_pos.franchise f \n    ON t.franchise_id = f.franchise_id\nLEFT JOIN frostbyte_tasty_bytes.raw_customer.customer_loyalty cl\n    ON s.customer_id = cl.customer_id;\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "641615b2-4935-4b17-9120-9bc7ac0d6032",
   "metadata": {
    "name": "cell10",
    "collapsed": false
   },
   "source": "### Snowflake Data Management Features\n\n1. **Zero-Copy Cloning**\n   - Creates instant copy of table without duplicating storage\n   - Only new/modified data consumes additional space\n   - Perfect for testing, development, and backup scenarios\n   - Syntax: `CREATE TABLE new_table CLONE source_table`\n\n2. **Time Travel**\n   - Allows querying historical data states\n   - Access data from a specific point in time\n   - No need to restore backups\n   - Syntax: `SELECT * FROM table AT (TIMESTAMP => time_expression)`\n   - Default retention: 24 hours (Enterprise: up to 90 days)"
  },
  {
   "cell_type": "code",
   "id": "5ddf286b-229d-4d82-abd1-bd286f7bc2d0",
   "metadata": {
    "language": "sql",
    "name": "cell9"
   },
   "outputs": [],
   "source": "-- =========================================\n-- 4. Zero-Copy Cloning\n-- =========================================\n-- Instantly create a copy of a table without consuming extra storage.\nUSE ROLE ACCOUNTADMIN;\nCREATE OR REPLACE TABLE FROSTBYTE_TASTY_BYTES.RAW_CUSTOMER.CUSTOMER_LOYALTY_DEV\nCLONE FROSTBYTE_TASTY_BYTES.RAW_CUSTOMER.CUSTOMER_LOYALTY;\n\n",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ec68677e-090c-40a7-ac82-846ef6a92e0e",
   "metadata": {
    "name": "cell6",
    "collapsed": false
   },
   "source": "### Use Case 4\n\nDescription:Demonstrate how to identify the root cause from notifications in a pipeline failure.\n\nLet's go into the UI to look at our dynamic tables and pipeline failures. "
  },
  {
   "cell_type": "markdown",
   "id": "1d92b9b8-c220-46ed-a7c4-1f9e2a1eeb1b",
   "metadata": {
    "name": "cell8",
    "collapsed": false
   },
   "source": "### Use Case 8\nDescription: Demonstrate if and how we can set policies to enable data to be cached. Demonstrate if the platform provides ways to automatically optimize memory or storage.\n\nSnowflake has three built-in mechanisms for caching:\n- Result Cache: Stores query results for 24 hours\n- Metadata Cache: Stores table structure info\n- Data Cache: Stores temp data in warehouse local storage\n\nAll caches are automatically managed by Snowflake.\n\nIn the example below, we use Snowpark (Python for Snowflake). If we run the same query again, we use the RESULT CACHE. This does not incur a compute cost."
  },
  {
   "cell_type": "code",
   "id": "5325aaee-f0fa-4764-afc0-18a132231829",
   "metadata": {
    "language": "python",
    "name": "cell4"
   },
   "outputs": [],
   "source": "from snowflake.snowpark.functions import col\nfrom snowflake.snowpark.context import get_active_session\n\nsession = get_active_session()\ndf = session.table('FROSTBYTE_TASTY_BYTES.HARMONIZED.ORDERS_ENRICHED_DT') \\\n    .select(col('menu_type'), col('region')) \\\n    .groupBy('menu_type', 'region') \\\n    .count()\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43aa4c84-147d-446d-b0d2-6d6d1f9c700d",
   "metadata": {
    "language": "python",
    "name": "cell7"
   },
   "outputs": [],
   "source": "#Running it again\n\ndf.show()",
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "51bc50b6-18b9-49d8-9ada-d97d5d4ffd34",
   "metadata": {
    "name": "cell5",
    "collapsed": false
   },
   "source": "We can also go into the query history to see whether the query used a warehouse or used a cache."
  }
 ]
}